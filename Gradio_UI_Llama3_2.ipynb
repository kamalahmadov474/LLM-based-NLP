{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kamalahmadov474/LLM-based-NLP/blob/main/Gradio_UI_Llama3_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46ea7618-795c-4a9b-951a-022b4dda0c28",
      "metadata": {
        "id": "46ea7618-795c-4a9b-951a-022b4dda0c28"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "\n",
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from typing import List\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "import google.generativeai\n",
        "import anthropic\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1fa1741-ff0f-48f6-ae4e-679359e5e050",
      "metadata": {
        "id": "e1fa1741-ff0f-48f6-ae4e-679359e5e050",
        "outputId": "4c9976fc-40d2-4900-ffab-54dd5c3fcc21"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest â ‹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest â ™ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest â ¹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest â ¸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest â ¼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest â ´ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest â ¦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
            "pulling dde5aa3fc5ff: 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–� 2.0 GB                         \u001b[K\n",
            "pulling 966de95ca8a6: 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–� 1.4 KB                         \u001b[K\n",
            "pulling fcc5a6bec9da: 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–� 7.7 KB                         \u001b[K\n",
            "pulling a70ff7e570d9: 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–� 6.0 KB                         \u001b[K\n",
            "pulling 56bb8bd477a5: 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–�   96 B                         \u001b[K\n",
            "pulling 34bb5ab01051: 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–�  561 B                         \u001b[K\n",
            "verifying sha256 digest \u001b[K\n",
            "writing manifest \u001b[K\n",
            "success \u001b[K\u001b[?25h\u001b[?2026l\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2 + 2 = 4.\n"
          ]
        }
      ],
      "source": [
        "!ollama pull llama3.2\n",
        "\n",
        "from openai import OpenAI\n",
        "MODEL = \"llama3.2\"\n",
        "openai = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
        "\n",
        "response = openai.chat.completions.create(\n",
        " model=MODEL,\n",
        " messages=[{\"role\": \"user\", \"content\": \"What is 2 + 2?\"}]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5545567b-c128-4d8e-a720-bef134dde109",
      "metadata": {
        "id": "5545567b-c128-4d8e-a720-bef134dde109",
        "outputId": "989fcf6f-e69e-4382-9940-b0771d0f7214"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello! Welcome to our little chat! I'm thrilled to meet you for the very first time. It's great that you're taking the first step and saying hello - I'll do my best to help and chat with you in a friendly way. How's your day going so far? Is there something specific on your mind, or do you just want to see where the conversation takes us?\n"
          ]
        }
      ],
      "source": [
        "# To give you a preview -- calling OpenAI with these messages is this easy. Any problems, head over to the Troubleshooting notebook.\n",
        "\n",
        "message = \"Hello, GPT! This is my first ever message to you! Hi!\"\n",
        "response = openai.chat.completions.create(model=\"llama3.2\", messages=[{\"role\":\"user\", \"content\":message}])\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79c72575-a8f8-4ac4-81dc-65750c7942fd",
      "metadata": {
        "id": "79c72575-a8f8-4ac4-81dc-65750c7942fd"
      },
      "outputs": [],
      "source": [
        "# A generic system message - no more snarky adversarial AIs!\n",
        "\n",
        "system_message = \"You are a helpful assistant\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "375073bf-bddc-430a-92e5-eb798fb5e7d3",
      "metadata": {
        "id": "375073bf-bddc-430a-92e5-eb798fb5e7d3"
      },
      "outputs": [],
      "source": [
        "# Let's wrap a call to Llama3.2 in a simple function\n",
        "\n",
        "def message_Llama(prompt):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "      ]\n",
        "    completion = openai.chat.completions.create(\n",
        "        model='llama3.2',\n",
        "        messages=messages,\n",
        "    )\n",
        "    return completion.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3bc616d-c271-45b6-874f-18310c5984b2",
      "metadata": {
        "id": "b3bc616d-c271-45b6-874f-18310c5984b2"
      },
      "outputs": [],
      "source": [
        "# here's a simple function\n",
        "\n",
        "def shout(text):\n",
        "    print(f\"Shout has been called with input {text}\")\n",
        "    return text.upper()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "746cf0c3-4a18-489d-8aa9-22be268d3ca8",
      "metadata": {
        "id": "746cf0c3-4a18-489d-8aa9-22be268d3ca8",
        "outputId": "ed03f32a-b184-4d7c-af9f-a9c493c91c37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shout has been called with input hello\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'HELLO'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "shout(\"hello\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bcf684c-5198-4658-8bb9-36713861d267",
      "metadata": {
        "id": "0bcf684c-5198-4658-8bb9-36713861d267",
        "outputId": "08266111-2e4e-4b1f-8d46-c54f55f3ce46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7862\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shout has been called with input hello\n",
            "Shout has been called with input llm\n",
            "Shout has been called with input llama\n"
          ]
        }
      ],
      "source": [
        "# The simplicty of gradio. This might appear in \"light mode\" - I'll show you how to make this in dark mode later.\n",
        "\n",
        "gr.Interface(fn=shout, inputs=\"textbox\", outputs=\"textbox\").launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9abdd10f-a0c4-444a-8f60-92a7db683a97",
      "metadata": {
        "id": "9abdd10f-a0c4-444a-8f60-92a7db683a97",
        "outputId": "cf5a176b-1219-4cb0-c238-e394886f7148"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7863\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shout has been called with input llm\n",
            "Shout has been called with input gradio\n"
          ]
        }
      ],
      "source": [
        "# Define this variable and then pass js=force_dark_mode when creating the Interface\n",
        "\n",
        "force_dark_mode = \"\"\"\n",
        "function refresh() {\n",
        "    const url = new URL(window.location);\n",
        "    if (url.searchParams.get('__theme') !== 'dark') {\n",
        "        url.searchParams.set('__theme', 'dark');\n",
        "        window.location.href = url.href;\n",
        "    }\n",
        "}\n",
        "\"\"\"\n",
        "gr.Interface(fn=shout, inputs=\"textbox\", outputs=\"textbox\", flagging_mode=\"never\", js=force_dark_mode).launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b53be8d-9bfa-4b97-9bd5-a88c54b78971",
      "metadata": {
        "id": "3b53be8d-9bfa-4b97-9bd5-a88c54b78971",
        "outputId": "df28d198-e2e0-4bf5-f886-19f47a494fe6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7864\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shout has been called with input llm\n",
            "Shout has been called with input llama\n",
            "Shout has been called with input gradio\n"
          ]
        }
      ],
      "source": [
        "# Inputs and Outputs\n",
        "\n",
        "view = gr.Interface(\n",
        "    fn=shout,\n",
        "    inputs=[gr.Textbox(label=\"Your message:\", lines=6)],\n",
        "    outputs=[gr.Textbox(label=\"Response:\", lines=8)],\n",
        "    flagging_mode=\"never\"\n",
        ")\n",
        "view.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "242da5f9-6cc4-447b-9e8f-2ab64ef4ea6d",
      "metadata": {
        "id": "242da5f9-6cc4-447b-9e8f-2ab64ef4ea6d",
        "outputId": "e21eeab2-5c0a-49a3-beea-24d0baf38e1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7872\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7872/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# And now - changing the function from \"shout\" to \"message_Llama\"\n",
        "\n",
        "view = gr.Interface(\n",
        "    fn=message_Llama,\n",
        "    inputs=[gr.Textbox(label=\"Your message:\", lines=6)],\n",
        "    outputs=[gr.Textbox(label=\"Response:\", lines=8)],\n",
        "    flagging_mode=\"never\"\n",
        ")\n",
        "view.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79a7ce3b-b2e8-4750-9428-1cbe1cbb3304",
      "metadata": {
        "id": "79a7ce3b-b2e8-4750-9428-1cbe1cbb3304",
        "outputId": "2b8edf0a-4dc7-41c9-f785-c78d4819408b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7873\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7873/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let's use Markdown\n",
        "# Are you wondering why it makes any difference to set system_message when it's not referred to in the code below it?\n",
        "# I'm taking advantage of system_message being a global variable, used back in the message_gpt function (go take a look)\n",
        "# Not a great software engineering practice, but quite common during Jupyter Lab R&D!\n",
        "\n",
        "system_message = \"You are a helpful assistant that responds in markdown\"\n",
        "\n",
        "view = gr.Interface(\n",
        "    fn=message_Llama,\n",
        "    inputs=[gr.Textbox(label=\"Your message:\")],\n",
        "    outputs=[gr.Markdown(label=\"Response:\")],\n",
        "    flagging_mode=\"never\"\n",
        ")\n",
        "view.launch()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}